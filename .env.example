# Namunify Configuration
# Copy this file to .env and fill in your values

# LLM Provider: openai or anthropic
NAMUNIFY_LLM_PROVIDER=openai

# LLM Model name
# OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo
# Anthropic: claude-sonnet-4-6-20250514, claude-opus-4-6-20250514, claude-haiku-4-5-20251001
NAMUNIFY_LLM_MODEL=gpt-4o

# API Key (required)
# For OpenAI, you can also set OPENAI_API_KEY
# For Anthropic, you can also set ANTHROPIC_API_KEY
NAMUNIFY_LLM_API_KEY=your-api-key-here

# Custom API Base URL (optional, for proxies or custom endpoints)
# NAMUNIFY_LLM_BASE_URL=https://api.openai.com/v1

# Maximum tokens for LLM response
NAMUNIFY_LLM_MAX_TOKENS=4096

# Temperature for LLM generation (0.0 - 1.0)
NAMUNIFY_LLM_TEMPERATURE=0.3

# Maximum context size in characters
NAMUNIFY_MAX_CONTEXT_SIZE=32000

# Maximum symbols to rename in one batch
NAMUNIFY_MAX_SYMBOLS_PER_BATCH=50

# Include full scope details in debug logs (very verbose)
NAMUNIFY_DEBUG_SCOPE_DETAILS=false

# Enable strict batching for top-level (program) scope
NAMUNIFY_PROGRAM_BATCHING_ENABLED=true

# Maximum top-level symbols per LLM call
NAMUNIFY_PROGRAM_MAX_SYMBOLS_PER_BATCH=10

# Top-level var/let/const assignment must be small to be batch-eligible
NAMUNIFY_PROGRAM_VARIABLE_MAX_ASSIGNMENT_CHARS=120
NAMUNIFY_PROGRAM_VARIABLE_MAX_ASSIGNMENT_LINES=2

# Top-level function declaration must be small to be batch-eligible
NAMUNIFY_PROGRAM_FUNCTION_MAX_CHARS=600
NAMUNIFY_PROGRAM_FUNCTION_MAX_LINES=20

# Lines of context around each symbol
NAMUNIFY_CONTEXT_PADDING=500

# Output directory (optional)
# NAMUNIFY_OUTPUT_DIR=./output

# Apply prettier formatting to output
NAMUNIFY_PRETTIER_FORMAT=true

# Preserve comments in output
NAMUNIFY_PRESERVE_COMMENTS=true

# Unpack webpack bundles automatically
NAMUNIFY_UNPACK_WEBPACK=true
